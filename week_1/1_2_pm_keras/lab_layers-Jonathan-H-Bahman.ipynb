{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Layers Lab\n",
    "\n",
    "Welcome to the Neural Networks Layers lab! By the end of this lab, you will have\n",
    "\n",
    "- Implemented Affine, ReLU, and Squared Loss layers\n",
    "- Built a modular implementation of a one-hidden layer perceptron model with a squared loss\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "# Neural Network Layers\n",
    "\n",
    "A neural network *layer* is a unit of computation that knows how to\n",
    "\n",
    "- Compute a *forward* pass to computed output(s) from its input(s)\n",
    "- Compute a *backward* pass to compute its input gradient(s) are from its output gradient(s) \n",
    "\n",
    "A neural network *layer* can be implemented in many ways, one reasonable choice being a python class which conforms to the following interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError('Forward pass not implemented!')\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        raise NotImplementedError('Backward pass not implemented!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "Let's start off simple with a Plus layer.\n",
    "\n",
    "<img src=\"images/Plus.svg\" alt=\"Plus Layer\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Plus(Layer):\n",
    "    def forward(self, a, b):\n",
    "        c = a + b\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        da, db = 1*dc, 1*dc\n",
    "        return da, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the Plus layer computes its output c given its inputs `a` and `b`. The Plus layer also computes its input gradients `da` and `db` given its output gradient `dc`.\n",
    "\n",
    "Notice that `Plus.backward()` does not need to know the value of `a` nor `b` (i.e. its inputs). Such a layer is called *stateless* because it doesn't need to remember anything from its forward pass.\n",
    "\n",
    "In contrast, some layers are *stateful*. A stateful layer requires knowledge of values that were computed during its forward pass in order to compute its backward pass. The Square layer is an example of a stateful layer.\n",
    "\n",
    "<img src=\"images/Square.svg\" alt=\"Square Layer\" style=\"width: 400px;\"/>\n",
    "\n",
    "Notice the Square layer is stateful because $\\dfrac{\\partial y}{\\partial x}$ is a function of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Square(Layer):\n",
    "    def forward(self, x):\n",
    "        y = x**2\n",
    "        self.cache = locals()\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        x = self.cache['x']\n",
    "        dx = 2*x * dy\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retain knowledge of values computed during the forward pass of the Square layer, we call the python builtin `locals()` function right before exiting. The `locals()` function returns a `dict` containing all of the local variables in the current scope. It's basically a very convenient way to quickly record everything that's been computed in the forward pass. We save it to an attribute `self.cache` so that we can retrieve it in the backward pass.\n",
    "\n",
    "Once we have layers, we can chain them together to form a more interesting computational graphs. Here's an example of chaining together a `Plus` layer and a `Square` layer.\n",
    "\n",
    "<img src=\"images/Pipeline.svg\" alt=\"Pipeline\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus, square = Plus(), Square()\n",
    "\n",
    "a, b = 3, 2\n",
    "\n",
    "c = plus.forward(a, b)\n",
    "y = square.forward(c)\n",
    "\n",
    "dy = 1\n",
    "dc = square.backward(dy)\n",
    "da, db = plus.backward(dc)\n",
    "\n",
    "da, db, dc, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, an invocation of a computational graph consists of four steps.\n",
    "\n",
    "1. Instantiate the layers of your computational graph\n",
    "2. Define the inputs values of the inputs to the graph\n",
    "3. Perfrom the forward pass\n",
    "4. Perform the backward pass (i.e. backpropagation)\n",
    "\n",
    "Enough examples. It's your turn to implement some layers!\n",
    "\n",
    "---\n",
    "\n",
    "# Affine Layer\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Implement an Affine layer which computes the function\n",
    "\n",
    "$$\n",
    "\\text{Affine}(x, w, b) = wx + b\n",
    "$$\n",
    "\n",
    "and hence corresponds to the computational graph\n",
    "\n",
    "<img src=\"images/Affine Abstraction Black Box.svg\" alt=\"Affine Black Box\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in the computational graph\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Implement the Affine layer in terms of operations which have simple local gradients (i.e. are easy to backpropagate through) as in the computational graph\n",
    "\n",
    "<img src=\"images/Affine Abstraction White Box.svg\" alt=\"Affine White Box\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Questions\n",
    "\n",
    "- Why do you think we compute $\\nabla_x$? Recall in the previous lab, we only computed $\\nabla_w$ and $\\nabla_b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine(Layer):\n",
    "    def forward(self, x, w, b):\n",
    "        \n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.a = x*w+b\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        db = 1 * da\n",
    "        dz = 1 * da\n",
    "        dw = self.x*dz\n",
    "        dx = self.w*dz\n",
    "        \n",
    "        \n",
    "        return dx, dw, dz, db, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affine = Affine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, w, b = 1,2,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine.forward(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 1, 1, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine.backward(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "- Implement a Rectified Linear Unit (ReLU) layer which computes the function\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(a) = \\begin{cases} 0 & \\text{if } a < 0 \\\\ a & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "and hence corresponds to the computational graph\n",
    "\n",
    "<img src=\"images/ReLU Layer Black Box.svg\" alt=\"ReLU Layer Black Box\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in the computational graph\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Implement the ReLU layer in terms of operations which have simple local gradients (i.e. are easy to backpropagate through) as in the computational graph\n",
    "\n",
    "<img src=\"images/ReLU Layer White Box.svg\" alt=\"ReLU Layer White Box\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReLu(Layer):\n",
    "    def forward(self, a):\n",
    "        \n",
    "        self.a = a\n",
    "        self.h = max(self.a,0)\n",
    "        return self.h\n",
    "    \n",
    "    def backward(self,dh):\n",
    "        if self.a > 0:\n",
    "            dh = 1\n",
    "        else:\n",
    "            dh = 0\n",
    "        da = 1 * dh\n",
    "        return da, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relu = ReLu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu.forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared Loss Layer\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Implement a SquaredLoss layer which computes the function\n",
    "\n",
    "$$\n",
    "\\text{SquaredLoss}(\\hat{y}, y) = (\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "and hence corresponds to the computational graph\n",
    "\n",
    "<img src=\"images/Squared Loss Black Box.svg\" alt=\"Squared Loss Black Box\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the exact variable names as used in the computational graph\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Implement the SquaredLoss layer in terms of operations which have simple local gradients (i.e. are easy to backpropagate through) as in the computational graph\n",
    "\n",
    "<img src=\"images/Squared Loss White Box.svg\" alt=\"Squared Loss White Box\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SquaredLoss(Layer):\n",
    "    def forward(self,y_hat,y):\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        self.l = (self.y_hat - self.y)**2\n",
    "        return self.l\n",
    "    def backward(self,dl):\n",
    "        dl = 1\n",
    "        r = self.y_hat - self.y\n",
    "        dr = 2*r*dl\n",
    "        d_yhat = 1 * dr\n",
    "        return d_yhat, dr, dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squarredloss = SquaredLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squarredloss.forward(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dl = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2, -2, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squarredloss.backward(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layered One-Hidden-Layer Neural Network\n",
    "\n",
    "Recall from the last lab a one-hidden-layer neural network model takes the form\n",
    "\n",
    "$$\n",
    "g(x, w_1, b_1, w_2, b_2) = \\max(\\max(w_1 x + b_1, 0)w_2 + b_2, 0).\n",
    "$$\n",
    "\n",
    "Rewriting $g$ in terms of the layers we have defined yields\n",
    "\n",
    "$$\n",
    "g(x, w_1, b_1, w_2, b_2) = \\text{Affine}(\\text{ReLU}(\\text{Affine}(x, w_1, b_1)), w_2, b_2).\n",
    "$$\n",
    "\n",
    "Applying a squared loss to $g$ yields the loss function\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(x, y, w_1, b_1, w_2, b_2)\n",
    "&= \\text{SquaredLoss}(\\text{Affine}(\\text{ReLU}(\\text{Affine}(x, w_1, b_1)), w_2, b_2), y)\n",
    "\\end{align*}\n",
    "\n",
    "for a given $(x, y)$ training pair and parameters $(w_1, b_1, w_2, b_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute $\\mathcal{L}(2, 1, -1, 1, -2, 1.5)$ as corresponding to the computational graph\n",
    "\n",
    "<img src=\"images/MLP Layers Numeric Forward.svg\" alt=\"MLP Layers Forward Numeric\" style=\"width: 1000px;\"/>\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use only the layers you have defined in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN(Layer):\n",
    "    def forward(self,x,w1,b1,w2,b2,y):\n",
    "        self.a1 = Affine()\n",
    "        a1 = self.a1.forward(x,w1,b1)\n",
    "        \n",
    "        self.relu1 = ReLu()\n",
    "        h = self.relu1.forward(a1)\n",
    "        \n",
    "        self.a2 = Affine()\n",
    "        y_hat = self.a2.forward(h,w2,b2)\n",
    "        \n",
    "        self.square_loss = SquaredLoss()\n",
    "        l = self.square_loss.forward(y_hat,y)\n",
    "        return l\n",
    "        \n",
    "    def backward(self,dl):\n",
    "        \"\"\"Compute both local and global gradients, Variables denoted with _ at the end are global gradients.\"\"\"\n",
    "        dl_ = dl\n",
    "        d_yhat_ , dr, dl = self.square_loss.backward(dl_)\n",
    "        \n",
    "        dh_, dw2_, dz2, db2_, da2 = self.a2.backward(d_yhat_) #da is d_yhat here\n",
    "        \n",
    "        \n",
    "        da_, dh =  self.relu1.backward(dh_)\n",
    "        dx, dw1_, dz1, db1_, da1 =   self.a1.backward(da_)\n",
    "        \n",
    "        return dw1_, db1_, da_, dh_, dw2_,db2_, d_yhat_ , dl_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_network = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = simple_network.forward(x=2,w1=-1,b1=1,w2=-2,b2=1.5,y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, -2.0, 0.0, 1.0, 1.0, 1)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_network.backward(dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- Compute $\\nabla_{w_1}$, $\\nabla_{b_1}$, $\\nabla_{w_2}$, and $\\nabla_{b_2}$ as corresponding to the computational graph\n",
    "\n",
    "<img src=\"images/MLP Layers Numeric Backward.svg\" alt=\"MLP Layers Backward Numeric\" style=\"width: 1000px;\"/>\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Use the variable naming convention `d`$\\cdot = \\overset{\\longleftarrow}{\\nabla_\\cdot}$ For example, $\\overset{\\longleftarrow}{\\nabla_r}$ gets the variable name `dr`.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- $\\overset{\\longleftarrow}{\\nabla_\\ell}$ = 1 will get you started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "- Did you need to make a call to `locals()` to cache anything during the forward pass? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> - Yes, you need to cache the instantiations of the previous class to call them for the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Tasks\n",
    "\n",
    "- Implement a Sigmoid layer\n",
    "- Implement a Softamx layer\n",
    "- Implement a hinge loss layer\n",
    "- Implement a vectorized Affine layer\n",
    "- Implement a vectorized ReLU layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sigmoid Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, w, x, b):\n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        a = w*x ## weights times x\n",
    "        c = a-b # subtract the bias\n",
    "        self.h = -1*c # turn negative\n",
    "        self.den = 1+ np.exp(self.h)\n",
    "        num = 1\n",
    "        return num / self.den\n",
    "    \n",
    "\n",
    "        \n",
    "    def backward(self, dl):\n",
    "        dden = dl*(-1/self.den**2)\n",
    "        df = 1*dden\n",
    "        dh = np.exp(self.h)*df\n",
    "        dz = -1*dh\n",
    "        db = 1*dz\n",
    "        da = 1*dz\n",
    "        dw = self.x*da\n",
    "        return db, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w,x,b = 1, -4, -6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88079707797788231"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid.forward(w,x,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10499358540350651, -0.41997434161402603)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid.backward(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a vectorized Affine layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine_Matrix(Layer):\n",
    "    def forward(self, X, w, b):\n",
    "        \n",
    "        self.X = X\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.a = X @ w.T +b\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        db = 1 * da\n",
    "        dz = 1 * da\n",
    "        dw = self.x*dz\n",
    "        dx = self.w*dz\n",
    "        \n",
    "        \n",
    "        return dx, dw, dz, db, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,2],\n",
    "             [2,4],\n",
    "             [4,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([[1],\n",
    "             [2],\n",
    "             [3]])\n",
    "b = np.array([[5],\n",
    "             [2],\n",
    "             [-3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affine_matrix = Affine_Matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affine_matrix.forward(Xmw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
